import hashlib
import re
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse
from uuid import uuid4

from bs4 import BeautifulSoup

from scraper.rate_limiter import fetch
from scraper.user_scraper import (get_or_fetch_user, extract_user_id_from_profile_url)

BASE_URL = "https://www.personalitycafe.com"

# Thread index selectors
THREAD_CARD_SELECTOR = "div.structItem--thread"
THREAD_LINK_SELECTOR = "h3.structItem-title a"

# Post selectors
POST_SELECTOR = "article.js-post"
USERNAME_SELECTOR = ".MessageCard__user-info__name"
BODY_SELECTOR = ".message-body .bbWrapper"
QUOTE_BLOCK_SELECTOR = "blockquote.bbCodeBlock--quote"
QUOTE_SOURCE_LINK_SELECTOR = ".bbCodeBlock-sourceJump"


def absolute_url(href: str) -> str:
    if href.startswith("http"):
        return href
    if href.startswith("/"):
        return BASE_URL + href
    return BASE_URL + "/" + href.lstrip("/")

def _is_member_link(href: str | None) -> bool:
    """Return True when href looks like a member profile link."""
    return bool(href and "/members/" in href)


def _current_scrape_timestamp() -> str:
    """Return an ISO8601 timestamp used for row-level bookkeeping."""
    return datetime.now().isoformat(timespec="seconds") + "Z"


def _thread_id_from_url(thread_url: str) -> str:
    """Derive a stable thread identifier from the thread URL."""
    path = urlparse(thread_url).path.rstrip("/")
    match = re.search(r"\.(\d+)$", path)
    if match:
        return match.group(1)
    match = re.search(r"(\d+)$", path)
    if match:
        return match.group(1)
    return hashlib.sha1(thread_url.encode("utf-8", "ignore")).hexdigest()[:16]


def _parse_post_id_from_quote_link(link) -> str | None:
    if not link:
        return None
    selector = link.get("data-content-selector")
    if selector:
        match = re.search(r"post-(\d+)", selector)
        if match:
            return match.group(1)
    href = link.get("href")
    if href:
        match = re.search(r"(?:id=|post-)(\d+)", href)
        if match:
            return match.group(1)
    return None


def _clean_quote_username(raw: str | None) -> str | None:
    if not raw:
        return None
    cleaned = raw.strip()
    cleaned = re.sub(r"\s*said:?$", "", cleaned, flags=re.IGNORECASE)
    return cleaned or None


def _extract_quote_targets(post_div) -> list[dict]:
    quotes: list[dict] = []
    for block in post_div.select(QUOTE_BLOCK_SELECTOR):
        link = block.select_one(QUOTE_SOURCE_LINK_SELECTOR)
        target_post_id = _parse_post_id_from_quote_link(link)
        username = _clean_quote_username(link.get_text(" ", strip=True) if link else None)
        if not target_post_id and not username:
            continue
        quotes.append({
            "target_post_id": target_post_id,
            "target_username": username,
        })
    return quotes


def _extract_mentions(body_el) -> list[dict]:
    mentions: list[dict] = []
    if not body_el:
        return mentions

    seen: set[tuple[str | None, str | None]] = set()
    for link in body_el.select("a"):
        href = link.get("href")
        if not href or "/members/" not in href:
            continue
        classes = link.get("class") or []
        if not link.get("data-user-id") and not any(cls.startswith("username") for cls in classes):
            continue
        profile_url = absolute_url(str(href))
        username = link.get_text(strip=True) or None
        key = (profile_url, username)
        if key in seen:
            continue
        seen.add(key)
        mentions.append({
            "profile_url": profile_url,
            "username": username,
            "user_id": extract_user_id_from_profile_url(profile_url),
        })
    return mentions

def get_thread_list(
    forum_url: str,
    max_pages: int | None = 3,
    thread_limit: int | None = 5,
):
    """
    Scrape a forum section index to collect thread URLs.
    `max_pages` prevents infinite crawling; increase carefully.
    `thread_limit` stops once N unique threads are gathered (default 5).
    """
    seen = set()
    ordered_threads: list[str] = []
    page_url = forum_url
    page = 1

    while True:
        print(f"[threads] Fetching forum index page {page}: {page_url}")
        html = fetch(page_url)
        soup = BeautifulSoup(html, "html.parser")

        for card in soup.select(THREAD_CARD_SELECTOR):
            link = card.select_one(THREAD_LINK_SELECTOR)
            if not link:
                continue
            href = link.get("href")
            if isinstance(href, list):
                href = href[0]
            if not href:
                continue
            url = absolute_url(str(href))
            if url in seen:
                continue
            seen.add(url)
            ordered_threads.append(url)
            if thread_limit is not None and len(ordered_threads) >= thread_limit:
                break

        if thread_limit is not None and len(ordered_threads) >= thread_limit:
            break
            
        if max_pages is not None and page >= max_pages:
            break

        # Find "next page" (if any)
        next_link = soup.find("a", rel="next")
        if not next_link:
            break
        next_href = next_link.get("href")
        if isinstance(next_href, list):
            next_href = next_href[0]
        if not next_href:
            break
        page_url = absolute_url(str(next_href))
        page += 1

    print(f"[threads] Collected {len(ordered_threads)} thread URLs.")
    return ordered_threads

def get_thread_pages(thread_url: str, max_pages: int | None = 20):
    """
    Return a list of URLs for all pages inside a thread.
    """
    pages = [thread_url]
    page_url = thread_url
    page = 1

    while True:
        if max_pages is not None and page >= max_pages:
            break

        next_page = page + 1
        print(f"[thread-pages] Checking for page {next_page} of thread {thread_url}")
        html = fetch(page_url)
        soup = BeautifulSoup(html, "html.parser")
        next_link = soup.find("a", rel="next")
        if not next_link:
            break
        next_href = next_link.get("href")
        if isinstance(next_href, list):
            next_href = next_href[0]
        if not next_href:
            break
        page_url = absolute_url(str(next_href))
        pages.append(page_url)
        page += 1

    return pages

def _extract_post_id(post_div) -> str | None:
    """
    Try to extract a stable post_id from attributes.
    Common patterns: data-content, id="js-post-123", id="post-123", etc.
    """
    # 1) data-content
    pid = post_div.get("data-content")
    if pid:
        return str(pid)

    # 2) id with digits
    elem_id = post_div.get("id")
    if elem_id:
        m = re.search(r"(\d+)$", elem_id)
        if m:
            return m.group(1)

    return None

def parse_posts_from_page(html: str):
    """
    Parse posts from one thread page.
    Returns a list of dicts:
      {
        "post_id",
        "username",
        "profile_url",
        "timestamp",
        "text",
      }
    """
    soup = BeautifulSoup(html, "html.parser")

    # Optional: debug HTML to inspect if selectors break
    digest = hashlib.sha1(html.encode("utf-8", "ignore")).hexdigest()
    debug_path = Path("debug_html") / f"page-{digest}.html"
    debug_path.parent.mkdir(parents=True, exist_ok=True)
    with open(debug_path, "w", encoding="utf-8") as debug_file:
        debug_file.write(soup.prettify())

    posts = []

    for post_div in soup.select(POST_SELECTOR):
        # Username
        user_el = post_div.select_one(USERNAME_SELECTOR)
        username = user_el.get_text(strip=True) if user_el else post_div.get("data-author")

        # Profile URL
        profile_url = None
        if user_el:
            link_el = user_el.find("a", href=True)
            if link_el:
                profile_url = absolute_url(str(link_el["href"]))
        if not profile_url:
            # Fallback: any link to /members/ inside post card
            link_el = post_div.find("a", href=_is_member_link)
            if link_el and link_el.get("href"):
                profile_url = absolute_url(str(link_el["href"]))

        # Timestamp
        time_el = post_div.find("time", attrs={"datetime": True}) or post_div.find("time")
        timestamp = time_el.get("datetime") if time_el else None

        # Body text
        body_el = post_div.select_one(BODY_SELECTOR)
        text = body_el.get_text("\n", strip=True) if body_el else None
        quotes = _extract_quote_targets(post_div)
        mentions = _extract_mentions(body_el)

        # Post ID
        post_id = _extract_post_id(post_div)

        posts.append({
            "post_id": post_id,
            "username": username,
            "profile_url": profile_url,
            "timestamp": timestamp,
            "text": text,
            "quotes": quotes,
            "mentions": mentions,
        })

    return posts


def _build_interactions_for_post(
    *,
    thread_id: str,
    post_row: dict,
    quotes: list[dict],
    mentions: list[dict],
    post_author_index: dict[str, dict],
) -> list[dict]:
    interactions: list[dict] = []
    replying_post_id = post_row.get("post_id")
    if not replying_post_id:
        return interactions

    source_user_id = post_row.get("user_id")
    scraped_at = post_row.get("scraped_at")

    for quote in quotes:
        target_post_id = quote.get("target_post_id")
        target_user_id = None
        if target_post_id and target_post_id in post_author_index:
            target_user_id = post_author_index[target_post_id].get("user_id")
        interactions.append({
            "interaction_id": str(uuid4()),
            "replying_post_id": replying_post_id,
            "target_post_id": target_post_id,
            "source_user_id": source_user_id,
            "target_user_id": target_user_id,
            "thread_id": thread_id,
            "interaction_type": "quote",
            "confidence": 1.0,
            "scraped_at": scraped_at,
        })

    for mention in mentions:
        profile_url = mention.get("profile_url")
        target_user_id = mention.get("user_id")
        if not target_user_id and profile_url:
            target_user_id = extract_user_id_from_profile_url(profile_url)
        if not target_user_id and not mention.get("username"):
            continue
        interactions.append({
            "interaction_id": str(uuid4()),
            "replying_post_id": replying_post_id,
            "target_post_id": None,
            "source_user_id": source_user_id,
            "target_user_id": target_user_id,
            "thread_id": thread_id,
            "interaction_type": "mention",
            "confidence": 0.7,
            "scraped_at": scraped_at,
        })

    return interactions

def scrape_thread(
    thread_url: str,
    user_cache: dict[str, dict],
    max_pages: int,
    forum_url: str | None = None,
):
    """
    Scrape all posts in a thread, enrich with user metadata, and derive interactions.
    Returns (posts, interactions, thread_row).
    """
    all_posts: list[dict] = []
    interactions: list[dict] = []
    post_author_index: dict[str, dict] = {}
    thread_id = _thread_id_from_url(thread_url)
    thread_scrape_ts = _current_scrape_timestamp()
    pages = get_thread_pages(thread_url, max_pages=max_pages)

    for page_url in pages:
        print(f"[scrape-thread] Fetching page {page_url}")
        html = fetch(page_url)
        page_posts = parse_posts_from_page(html)

        for p in page_posts:
            profile_url = p.get("profile_url")
            user_id = None
            username = p.get("username")
            quotes = p.get("quotes") or []
            mentions = p.get("mentions") or []

            if profile_url:
                user = get_or_fetch_user(profile_url, user_cache)
                if user:
                    user_id = user["user_id"]
                    # Prefer canonical username from profile if present
                    if user.get("username"):
                        username = user["username"]
                else:
                    # fallback: derive from URL
                    user_id = extract_user_id_from_profile_url(profile_url)

            scraped_at = _current_scrape_timestamp()
            post_id = p.get("post_id")
            post_row = {
                "thread_id": thread_id,
                "thread_url": thread_url,
                "page_url": page_url,
                "post_id": post_id,
                "user_id": user_id,
                "username": username,
                "timestamp": p.get("timestamp"),
                "text": p.get("text"),
                "scraped_at": scraped_at,
            }
            all_posts.append(post_row)
            if post_id:
                post_author_index[post_id] = {
                    "user_id": user_id,
                    "username": username,
                }

            interactions.extend(
                _build_interactions_for_post(
                    thread_id=thread_id,
                    post_row=post_row,
                    quotes=quotes,
                    mentions=mentions,
                    post_author_index=post_author_index,
                )
            )

    thread_row = {
        "thread_id": thread_id,
        "thread_url": thread_url,
        "forum_url": forum_url,
        "first_seen": thread_scrape_ts,
        "last_seen": thread_scrape_ts,
        "scraped_at": thread_scrape_ts,
    }
    return all_posts, interactions, thread_row
